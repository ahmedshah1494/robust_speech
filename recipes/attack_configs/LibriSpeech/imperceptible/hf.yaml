# General information
seed: 1002
__set_seed: !apply:torch.manual_seed [!ref <seed>]
root: !PLACEHOLDER
tokenizers_folder: !ref <root>/tokenizers

# Hyparameters below are dependant on the attack and model used 
# and should be changed at the user's discretion
# -------------------------------------------------------------
# Attack information
eps: 1
target_sentence: ["THIS IS A SENTENCE OF MEDIUM LENGTH THAT WE USE AS TARGET", "A SHORT TARGET SENTENCE", "HERE IS A RATHER LONG TEST STRING THAT WE USE AS TARGET FOR THE LONGEST UTTERANCES IN LIBRISPEECH WHICH CAN CONTAIN DOZENS OF WORDS"]
attack_class: !name:robust_speech.adversarial.attacks.imperceptible.ImperceptibleASRAttack
   targeted: True
   decrease_factor_alpha: 0.5
   decrease_factor_eps: 0.5
   eps: !ref <eps>
   global_max_length: 562480
   increase_factor_alpha: 2.0
   initial_rescale: 1.0
   learning_rate_1: 0.002
   learning_rate_2: 0.000005
   max_iter_1: 1000
   max_iter_2: 4000
   num_iter_decrease_alpha: 50
   train_mode_for_backward: False
save_audio: False
attack_name: imperceptible

# Model information
model_repo: !PLACEHOLDER
model_name: !PLACEHOLDER
hf_model_name: !ref <model_repo>/<model_name>
target_brain_class: !name:robust_speech.models.sb_hf_binding.HuggingFaceASR
target_brain_hparams_file: !ref model_configs/hf_models/<model_name>.yaml
source_model_name: !ref <model_name>
source_brain_class: !ref <target_brain_class>
source_brain_hparams_file: !ref model_configs/hf_models/<model_name>.yaml

# Tokenizer information (compatible with target and source)
tokenizer: !apply:transformers.AutoTokenizer.from_pretrained
   - !ref <hf_model_name>

   # -------------------------------------------------------------

output_folder: !ref <root>/attacks/<attack_name>/<source_model_name>-<eps>/<seed>
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>
log: !ref <output_folder>/log.txt
save_audio_path: !ref <output_folder>/save

dataset_prepare_fct: !name:robust_speech.data.librispeech.prepare_librispeech
dataio_prepare_fct: !name:robust_speech.data.dataio.dataio_prepare

# Data files
data_folder: !ref <root>/data/LibriSpeech # e.g, /localscratch/LibriSpeech
csv_folder: !ref <data_folder>/csv # e.g, /localscratch/LibriSpeech
# If RIRS_NOISES dir exists in /localscratch/xxx_corpus/RIRS_NOISES
# then data_folder_rirs should be /localscratch/xxx_corpus
# otherwise the dataset will automatically be downloaded
test_splits: ["test-clean"]
skip_prep: True
ckpt_interval_minutes: 15 # save checkpoint every N min
data_csv_name: test-clean
test_csv:
   - !ref <data_folder>/csv/<data_csv_name>.csv
batch_size: 1 # This works for 2x GPUs with 32GB
avoid_if_longer_than: 14.0
sorting: ascending

# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 80

# Decoding parameters (only for text_pipeline)
blank_index: 0
bos_index: 1
eos_index: 2

test_dataloader_opts:
    batch_size: 1

logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <log>
    
